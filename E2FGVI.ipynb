{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZurXfq-F6jj"
      },
      "source": [
        "# Towards An <strong>E</strong>nd-to-<strong>E</strong>nd Framework for <strong>F</strong>low-<strong>G</strong>uided <strong>V</strong>ideo <strong>I</strong>npainting (CVPR 2022)\n",
        "\n",
        "In this demo, you can try to inpaint an example video through our framework."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDNKrW-NipaV"
      },
      "source": [
        "# Setup Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U4nBXDYiE-Y9",
        "outputId": "9cecfa7e-0aef-4639-a24e-dc1303c62663"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting torch==1.5.1+cu101\n",
            "  Downloading https://download.pytorch.org/whl/cu101/torch-1.5.1%2Bcu101-cp37-cp37m-linux_x86_64.whl (704.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 704.4 MB 1.3 kB/s \n",
            "\u001b[?25hCollecting torchvision==0.6.1+cu101\n",
            "  Downloading https://download.pytorch.org/whl/cu101/torchvision-0.6.1%2Bcu101-cp37-cp37m-linux_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 35.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.5.1+cu101) (1.21.6)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch==1.5.1+cu101) (0.16.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.6.1+cu101) (7.1.2)\n",
            "Installing collected packages: torch, torchvision\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.12.0+cu113\n",
            "    Uninstalling torch-1.12.0+cu113:\n",
            "      Successfully uninstalled torch-1.12.0+cu113\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.13.0+cu113\n",
            "    Uninstalling torchvision-0.13.0+cu113:\n",
            "      Successfully uninstalled torchvision-0.13.0+cu113\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchtext 0.13.0 requires torch==1.12.0, but you have torch 1.5.1+cu101 which is incompatible.\n",
            "torchaudio 0.12.0+cu113 requires torch==1.12.0, but you have torch 1.5.1+cu101 which is incompatible.\n",
            "fastai 2.7.7 requires torch<1.13,>=1.7, but you have torch 1.5.1+cu101 which is incompatible.\n",
            "fastai 2.7.7 requires torchvision>=0.8.2, but you have torchvision 0.6.1+cu101 which is incompatible.\u001b[0m\n",
            "Successfully installed torch-1.5.1+cu101 torchvision-0.6.1+cu101\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://download.openmmlab.com/mmcv/dist/cu101/torch1.5/index.html\n",
            "Collecting mmcv-full\n",
            "  Downloading https://download.openmmlab.com/mmcv/dist/cu101/torch1.5.0/mmcv_full-1.6.0-cp37-cp37m-manylinux1_x86_64.whl (41.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 41.9 MB 50.2 MB/s \n",
            "\u001b[?25hCollecting yapf\n",
            "  Downloading yapf-0.32.0-py2.py3-none-any.whl (190 kB)\n",
            "\u001b[K     |████████████████████████████████| 190 kB 8.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from mmcv-full) (21.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from mmcv-full) (3.13)\n",
            "Collecting addict\n",
            "  Downloading addict-2.4.0-py3-none-any.whl (3.8 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from mmcv-full) (1.21.6)\n",
            "Requirement already satisfied: opencv-python>=3 in /usr/local/lib/python3.7/dist-packages (from mmcv-full) (4.6.0.66)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from mmcv-full) (7.1.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->mmcv-full) (3.0.9)\n",
            "Installing collected packages: yapf, addict, mmcv-full\n",
            "Successfully installed addict-2.4.0 mmcv-full-1.6.0 yapf-0.32.0\n",
            "Cloning into 'E2FGVI'...\n",
            "remote: Enumerating objects: 342, done.\u001b[K\n",
            "remote: Counting objects: 100% (63/63), done.\u001b[K\n",
            "remote: Compressing objects: 100% (24/24), done.\u001b[K\n",
            "remote: Total 342 (delta 45), reused 39 (delta 39), pack-reused 279\u001b[K\n",
            "Receiving objects: 100% (342/342), 36.74 MiB | 28.70 MiB/s, done.\n",
            "Resolving deltas: 100% (54/54), done.\n"
          ]
        }
      ],
      "source": [
        "# Install Pytorch\n",
        "!pip install torch==1.5.1+cu101 torchvision==0.6.1+cu101 -f https://download.pytorch.org/whl/torch_stable.html \n",
        "# Install MMCV\n",
        "!pip install mmcv-full -f https://download.openmmlab.com/mmcv/dist/cu101/torch1.5/index.html\n",
        "\n",
        "# prepare code\n",
        "import os\n",
        "CODE_DIR = 'E2FGVI'\n",
        "os.makedirs(f'./{CODE_DIR}')\n",
        "!git clone https://github.com/MCG-NKU/E2FGVI.git $CODE_DIR\n",
        "os.chdir(f'./{CODE_DIR}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTYiBQVttie0"
      },
      "source": [
        "## Download Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "rbTtlHMtLih_"
      },
      "outputs": [],
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "import os\n",
        "\n",
        "download_with_pydrive = True\n",
        "\n",
        "class Downloader(object):\n",
        "    def __init__(self, use_pydrive):\n",
        "        self.use_pydrive = use_pydrive\n",
        "        current_directory = os.getcwd()\n",
        "        self.save_dir = os.path.join(os.path.dirname(current_directory), CODE_DIR, \"release_model\")\n",
        "        if not os.path.exists(self.save_dir):        \n",
        "            os.makedirs(self.save_dir)\n",
        "        if self.use_pydrive:\n",
        "            self.authenticate()\n",
        "\n",
        "    def authenticate(self):\n",
        "        auth.authenticate_user()\n",
        "        gauth = GoogleAuth()\n",
        "        gauth.credentials = GoogleCredentials.get_application_default()\n",
        "        self.drive = GoogleDrive(gauth)\n",
        "\n",
        "    def download_file(self, file_id, file_name):\n",
        "        file_dst = f'{self.save_dir}/{file_name}'\n",
        "        if os.path.exists(file_dst):\n",
        "            print(f'{file_name} already exists!')\n",
        "            return\n",
        "        downloaded = self.drive.CreateFile({'id':file_id})\n",
        "        downloaded.FetchMetadata(fetch_all=True)\n",
        "        downloaded.GetContentFile(file_dst)\n",
        "\n",
        "downloader = Downloader(download_with_pydrive)\n",
        "#path = {\"id\": \"1tNJMTJ2gmWdIXJoHVi5-H504uImUiJW9\", \"name\": \"E2FGVI_CVPR22_models.zip\"}\n",
        "#downloader.download_file(file_id=path[\"id\"], file_name=path[\"name\"])\n",
        "\n",
        "downloader.download_file('10wGdKSUOie0XmCr8SQ2A2FeDe-mfn5w3', 'E2FGVI-HQ-CVPR22.pth')\n",
        "\n",
        "#os.chdir(downloader.save_dir)\n",
        "#!unzip E2FGVI_CVPR22_models.zip\n",
        "#os.chdir('..')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hrwfKfhXVyjK"
      },
      "source": [
        "## Mount Input Data From Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X-jn2mqOV4KA",
        "outputId": "4aeb788f-1c09-4d3e-afca-70f9dc74d1c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wA7CC871cDnb"
      },
      "source": [
        "# Inpainting \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_LTPxX-HLh5"
      },
      "source": [
        "### Change Directory if need"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "76UWewrHwMCD"
      },
      "outputs": [],
      "source": [
        "## chdir if need\n",
        "import os\n",
        "CODE_DIR = 'E2FGVI'\n",
        "os.chdir(f'./{CODE_DIR}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EjgGEu7NHLh6"
      },
      "source": [
        "### Import modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "hQ-BltBjHLh6"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import importlib\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import animation\n",
        "import torch\n",
        "from core.utils import to_tensors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LzN_C2luHLh7"
      },
      "source": [
        "### Setup Global Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "e7OHv8p1HLh8"
      },
      "outputs": [],
      "source": [
        "# global variables\n",
        "ref_length = 10  # ref_step\n",
        "num_ref = 3\n",
        "neighbor_stride = 3\n",
        "default_fps = 24\n",
        "video_path = '/content/drive/MyDrive/video_inpating/input/delogo_examples/test_03.mp4'\n",
        "mask_path = '/content/drive/MyDrive/video_inpating/input/delogo_examples/mask/test_03_mask.png'\n",
        "use_mp4 = True if video_path.endswith('.mp4') else False\n",
        "ckpt = 'release_model/E2FGVI-HQ-CVPR22.pth'\n",
        "model_name = 'e2fgvi_hq'\n",
        "if model_name == 'e2fgvi_hq':\n",
        "    size = (960, 640) # 720p\n",
        "    # size = (1920, 1080)\n",
        "else:\n",
        "    size = (432, 240)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UN28l6yyHLh9"
      },
      "source": [
        "### Utility Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "yVriSFRztdtn"
      },
      "outputs": [],
      "source": [
        "# sample reference frames from the whole video\n",
        "def get_ref_index(f, neighbor_ids, length):\n",
        "    ref_index = []\n",
        "    if num_ref == -1:\n",
        "        for i in range(0, length, ref_length):\n",
        "            if i not in neighbor_ids:\n",
        "                ref_index.append(i)\n",
        "    else:\n",
        "        start_idx = max(0, f - ref_length * (num_ref // 2))\n",
        "        end_idx = min(length - 1, f + ref_length * (num_ref // 2))\n",
        "        for i in range(start_idx, end_idx + 1, ref_length):\n",
        "            if i not in neighbor_ids:\n",
        "                if len(ref_index) > num_ref:\n",
        "                    break\n",
        "                ref_index.append(i)\n",
        "    return ref_index\n",
        "\n",
        "\n",
        "# read frame-wise masks\n",
        "def read_mask(mpath, size):\n",
        "    masks = []\n",
        "    mnames = os.listdir(mpath)\n",
        "    mnames.sort()\n",
        "    for mp in mnames:\n",
        "        m = Image.open(os.path.join(mpath, mp))\n",
        "        m = m.resize(size, Image.NEAREST)\n",
        "        m = np.array(m.convert('L'))\n",
        "        m = np.array(m > 0).astype(np.uint8)\n",
        "        m = cv2.dilate(m,\n",
        "                       cv2.getStructuringElement(cv2.MORPH_CROSS, (3, 3)),\n",
        "                       iterations=4)\n",
        "        masks.append(Image.fromarray(m * 255))\n",
        "    return masks\n",
        "\n",
        "\n",
        "# read frame-wise masks\n",
        "def read_mask_lst(mpath, size, lst):\n",
        "    masks = []\n",
        "    mnames = os.listdir(mpath)\n",
        "    mnames.sort()\n",
        "    for i in lst:\n",
        "        #for mp in mnames:\n",
        "        mp = mnames[i]\n",
        "        m = Image.open(os.path.join(mpath, mp))\n",
        "        m = m.resize(size, Image.NEAREST)\n",
        "        m = np.array(m.convert('L'))\n",
        "        m = np.array(m > 0).astype(np.uint8)\n",
        "        m = cv2.dilate(m,\n",
        "                       cv2.getStructuringElement(cv2.MORPH_CROSS, (3, 3)),\n",
        "                       iterations=4)\n",
        "        masks.append(Image.fromarray(m * 255))\n",
        "    return masks\n",
        "\n",
        "\n",
        "def read_mask_static(mpath, size, n):\n",
        "    masks = []\n",
        "    m = Image.open(mpath)\n",
        "    m = m.resize(size, Image.NEAREST)\n",
        "    m = np.array(m.convert('L'))\n",
        "    m = np.array(m > 0).astype(np.uint8)\n",
        "    m = cv2.dilate(m,\n",
        "                   cv2.getStructuringElement(cv2.MORPH_CROSS, (3, 3)),\n",
        "                   iterations=4)\n",
        "    mm = Image.fromarray(m * 255)\n",
        "    for i in range(0, n):\n",
        "        masks.append(mm)\n",
        "    return masks\n",
        "\n",
        "\n",
        "def get_frame_count():\n",
        "    if use_mp4:\n",
        "        vidcap = cv2.VideoCapture(video_path)\n",
        "        length = int(vidcap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    else:\n",
        "        lst = os.listdir(video_path)\n",
        "        length = len(lst)\n",
        "    return length\n",
        "\n",
        "\n",
        "def read_frame_from_videos_by_index_list(index_lst):\n",
        "    frames = []\n",
        "    if use_mp4:\n",
        "        vidcap = cv2.VideoCapture(video_path)\n",
        "        for i in index_lst:\n",
        "            vidcap.set(cv2.CAP_PROP_POS_FRAMES, i)\n",
        "            success, image = vidcap.read()\n",
        "            if not success:\n",
        "                exit(1)\n",
        "            image = Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
        "            frames.append(image)\n",
        "    else:\n",
        "        lst = os.listdir(video_path)\n",
        "        lst.sort()\n",
        "        fr_lst = [video_path + '/' + name for name in lst]\n",
        "        for i in index_lst:\n",
        "            image = cv2.imread(fr_lst[i])\n",
        "            image = Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
        "            frames.append(image)\n",
        "    return frames\n",
        "\n",
        "\n",
        "#  read frames from video\n",
        "def read_frame_from_videos():\n",
        "    frames = []\n",
        "    if use_mp4:\n",
        "        vidcap = cv2.VideoCapture(video_path)\n",
        "        success, image = vidcap.read()\n",
        "        count = 0\n",
        "        while success:\n",
        "            image = Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
        "            frames.append(image)\n",
        "            success, image = vidcap.read()\n",
        "            count += 1\n",
        "    else:\n",
        "        lst = os.listdir(video_path)\n",
        "        lst.sort()\n",
        "        fr_lst = [video_path + '/' + name for name in lst]\n",
        "        for fr in fr_lst:\n",
        "            image = cv2.imread(fr)\n",
        "            image = Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
        "            frames.append(image)\n",
        "    return frames\n",
        "\n",
        "\n",
        "# resize frames\n",
        "def resize_frames(frames, size=None):\n",
        "    if size is not None:\n",
        "        frames = [f.resize(size) for f in frames]\n",
        "    else:\n",
        "        size = frames[0].size\n",
        "    return frames, size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtdB8rNmHLh_"
      },
      "source": [
        "## Main Wroker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "704c4b9337104692a4776c200bae12e7",
            "4b78baf38baa4d2db64899361ae29965",
            "e8c471a04e3046109811b42f47f80c88",
            "4cecdec88c774842b63760cb6d398bb7",
            "e33fd62ea20847f7a469df1a52c45bb0",
            "15e53b8a29d04fa488381b5b3916d4b1",
            "206ef8d2643a4c09934890197b019527",
            "b3f7d7a4aae84163803e83f729d072ca",
            "9720bcbfa95f4eb9a022ce037d742faf",
            "f0d71edb91814477b192a6ca97ac58f7",
            "faa7a70ea0a045f68314f8cdcedabf17"
          ]
        },
        "id": "MFVXcD3COD7B",
        "outputId": "a7b91996-7047-4d0f-f436-26cf5fb4663a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "load pretrained SPyNet...\n",
            "load checkpoint from http path: https://download.openmmlab.com/mmediting/restorers/basicvsr/spynet_20210409-c6c1bd09.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.openmmlab.com/mmediting/restorers/basicvsr/spynet_20210409-c6c1bd09.pth\" to /root/.cache/torch/checkpoints/spynet_20210409-c6c1bd09.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/5.50M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "704c4b9337104692a4776c200bae12e7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model from: release_model/E2FGVI-HQ-CVPR22.pth\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "InpaintGenerator(\n",
              "  (encoder): Encoder(\n",
              "    (layers): ModuleList(\n",
              "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "      (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
              "      (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
              "      (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "      (5): LeakyReLU(negative_slope=0.2, inplace=True)\n",
              "      (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
              "      (8): Conv2d(256, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (9): LeakyReLU(negative_slope=0.2, inplace=True)\n",
              "      (10): Conv2d(640, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2)\n",
              "      (11): LeakyReLU(negative_slope=0.2, inplace=True)\n",
              "      (12): Conv2d(768, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=4)\n",
              "      (13): LeakyReLU(negative_slope=0.2, inplace=True)\n",
              "      (14): Conv2d(640, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n",
              "      (15): LeakyReLU(negative_slope=0.2, inplace=True)\n",
              "      (16): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (17): LeakyReLU(negative_slope=0.2, inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (decoder): Sequential(\n",
              "    (0): deconv(\n",
              "      (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    )\n",
              "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
              "    (2): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
              "    (4): deconv(\n",
              "      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    )\n",
              "    (5): LeakyReLU(negative_slope=0.2, inplace=True)\n",
              "    (6): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  )\n",
              "  (feat_prop_module): BidirectionalPropagation(\n",
              "    (deform_align): ModuleDict(\n",
              "      (backward_): SecondOrderDeformableAlignment(\n",
              "        (conv_offset): Sequential(\n",
              "          (0): Conv2d(388, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
              "          (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (3): LeakyReLU(negative_slope=0.1, inplace=True)\n",
              "          (4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (5): LeakyReLU(negative_slope=0.1, inplace=True)\n",
              "          (6): Conv2d(128, 432, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        )\n",
              "      )\n",
              "      (forward_): SecondOrderDeformableAlignment(\n",
              "        (conv_offset): Sequential(\n",
              "          (0): Conv2d(388, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
              "          (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (3): LeakyReLU(negative_slope=0.1, inplace=True)\n",
              "          (4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (5): LeakyReLU(negative_slope=0.1, inplace=True)\n",
              "          (6): Conv2d(128, 432, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (backbone): ModuleDict(\n",
              "      (backward_): Sequential(\n",
              "        (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
              "        (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      )\n",
              "      (forward_): Sequential(\n",
              "        (0): Conv2d(384, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
              "        (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      )\n",
              "    )\n",
              "    (fusion): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "  )\n",
              "  (ss): SoftSplit(\n",
              "    (t2t): Unfold(kernel_size=(7, 7), dilation=1, padding=(3, 3), stride=(3, 3))\n",
              "    (embedding): Linear(in_features=6272, out_features=512, bias=True)\n",
              "  )\n",
              "  (sc): SoftComp(\n",
              "    (relu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
              "    (embedding): Linear(in_features=512, out_features=6272, bias=True)\n",
              "    (bias_conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  )\n",
              "  (transformer): Sequential(\n",
              "    (0): TemporalFocalTransformerBlock(\n",
              "      (pool_layers): ModuleList(\n",
              "        (0): Linear(in_features=45, out_features=1, bias=True)\n",
              "      )\n",
              "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      (attn): WindowAttention(\n",
              "        (unfolds): ModuleList(\n",
              "          (0): Unfold(kernel_size=(5, 9), dilation=1, padding=(2, 4), stride=1)\n",
              "        )\n",
              "        (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
              "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "        (softmax): Softmax(dim=-1)\n",
              "      )\n",
              "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      (mlp): FusionFeedForward(\n",
              "        (conv1): Sequential(\n",
              "          (0): Linear(in_features=512, out_features=1960, bias=True)\n",
              "        )\n",
              "        (conv2): Sequential(\n",
              "          (0): GELU()\n",
              "          (1): Linear(in_features=1960, out_features=512, bias=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (1): TemporalFocalTransformerBlock(\n",
              "      (pool_layers): ModuleList(\n",
              "        (0): Linear(in_features=45, out_features=1, bias=True)\n",
              "      )\n",
              "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      (attn): WindowAttention(\n",
              "        (unfolds): ModuleList(\n",
              "          (0): Unfold(kernel_size=(5, 9), dilation=1, padding=(2, 4), stride=1)\n",
              "        )\n",
              "        (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
              "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "        (softmax): Softmax(dim=-1)\n",
              "      )\n",
              "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      (mlp): FusionFeedForward(\n",
              "        (conv1): Sequential(\n",
              "          (0): Linear(in_features=512, out_features=1960, bias=True)\n",
              "        )\n",
              "        (conv2): Sequential(\n",
              "          (0): GELU()\n",
              "          (1): Linear(in_features=1960, out_features=512, bias=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (2): TemporalFocalTransformerBlock(\n",
              "      (pool_layers): ModuleList(\n",
              "        (0): Linear(in_features=45, out_features=1, bias=True)\n",
              "      )\n",
              "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      (attn): WindowAttention(\n",
              "        (unfolds): ModuleList(\n",
              "          (0): Unfold(kernel_size=(5, 9), dilation=1, padding=(2, 4), stride=1)\n",
              "        )\n",
              "        (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
              "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "        (softmax): Softmax(dim=-1)\n",
              "      )\n",
              "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      (mlp): FusionFeedForward(\n",
              "        (conv1): Sequential(\n",
              "          (0): Linear(in_features=512, out_features=1960, bias=True)\n",
              "        )\n",
              "        (conv2): Sequential(\n",
              "          (0): GELU()\n",
              "          (1): Linear(in_features=1960, out_features=512, bias=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (3): TemporalFocalTransformerBlock(\n",
              "      (pool_layers): ModuleList(\n",
              "        (0): Linear(in_features=45, out_features=1, bias=True)\n",
              "      )\n",
              "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      (attn): WindowAttention(\n",
              "        (unfolds): ModuleList(\n",
              "          (0): Unfold(kernel_size=(5, 9), dilation=1, padding=(2, 4), stride=1)\n",
              "        )\n",
              "        (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
              "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "        (softmax): Softmax(dim=-1)\n",
              "      )\n",
              "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      (mlp): FusionFeedForward(\n",
              "        (conv1): Sequential(\n",
              "          (0): Linear(in_features=512, out_features=1960, bias=True)\n",
              "        )\n",
              "        (conv2): Sequential(\n",
              "          (0): GELU()\n",
              "          (1): Linear(in_features=1960, out_features=512, bias=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (4): TemporalFocalTransformerBlock(\n",
              "      (pool_layers): ModuleList(\n",
              "        (0): Linear(in_features=45, out_features=1, bias=True)\n",
              "      )\n",
              "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      (attn): WindowAttention(\n",
              "        (unfolds): ModuleList(\n",
              "          (0): Unfold(kernel_size=(5, 9), dilation=1, padding=(2, 4), stride=1)\n",
              "        )\n",
              "        (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
              "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "        (softmax): Softmax(dim=-1)\n",
              "      )\n",
              "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      (mlp): FusionFeedForward(\n",
              "        (conv1): Sequential(\n",
              "          (0): Linear(in_features=512, out_features=1960, bias=True)\n",
              "        )\n",
              "        (conv2): Sequential(\n",
              "          (0): GELU()\n",
              "          (1): Linear(in_features=1960, out_features=512, bias=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (5): TemporalFocalTransformerBlock(\n",
              "      (pool_layers): ModuleList(\n",
              "        (0): Linear(in_features=45, out_features=1, bias=True)\n",
              "      )\n",
              "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      (attn): WindowAttention(\n",
              "        (unfolds): ModuleList(\n",
              "          (0): Unfold(kernel_size=(5, 9), dilation=1, padding=(2, 4), stride=1)\n",
              "        )\n",
              "        (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
              "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "        (softmax): Softmax(dim=-1)\n",
              "      )\n",
              "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      (mlp): FusionFeedForward(\n",
              "        (conv1): Sequential(\n",
              "          (0): Linear(in_features=512, out_features=1960, bias=True)\n",
              "        )\n",
              "        (conv2): Sequential(\n",
              "          (0): GELU()\n",
              "          (1): Linear(in_features=1960, out_features=512, bias=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (6): TemporalFocalTransformerBlock(\n",
              "      (pool_layers): ModuleList(\n",
              "        (0): Linear(in_features=45, out_features=1, bias=True)\n",
              "      )\n",
              "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      (attn): WindowAttention(\n",
              "        (unfolds): ModuleList(\n",
              "          (0): Unfold(kernel_size=(5, 9), dilation=1, padding=(2, 4), stride=1)\n",
              "        )\n",
              "        (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
              "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "        (softmax): Softmax(dim=-1)\n",
              "      )\n",
              "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      (mlp): FusionFeedForward(\n",
              "        (conv1): Sequential(\n",
              "          (0): Linear(in_features=512, out_features=1960, bias=True)\n",
              "        )\n",
              "        (conv2): Sequential(\n",
              "          (0): GELU()\n",
              "          (1): Linear(in_features=1960, out_features=512, bias=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (7): TemporalFocalTransformerBlock(\n",
              "      (pool_layers): ModuleList(\n",
              "        (0): Linear(in_features=45, out_features=1, bias=True)\n",
              "      )\n",
              "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      (attn): WindowAttention(\n",
              "        (unfolds): ModuleList(\n",
              "          (0): Unfold(kernel_size=(5, 9), dilation=1, padding=(2, 4), stride=1)\n",
              "        )\n",
              "        (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
              "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "        (softmax): Softmax(dim=-1)\n",
              "      )\n",
              "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      (mlp): FusionFeedForward(\n",
              "        (conv1): Sequential(\n",
              "          (0): Linear(in_features=512, out_features=1960, bias=True)\n",
              "        )\n",
              "        (conv2): Sequential(\n",
              "          (0): GELU()\n",
              "          (1): Linear(in_features=1960, out_features=512, bias=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (update_spynet): SPyNet(\n",
              "    (basic_module): ModuleList(\n",
              "      (0): SPyNetBasicModule(\n",
              "        (basic_module): Sequential(\n",
              "          (0): ConvModule(\n",
              "            (conv): Conv2d(8, 32, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
              "            (activate): ReLU(inplace=True)\n",
              "          )\n",
              "          (1): ConvModule(\n",
              "            (conv): Conv2d(32, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
              "            (activate): ReLU(inplace=True)\n",
              "          )\n",
              "          (2): ConvModule(\n",
              "            (conv): Conv2d(64, 32, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
              "            (activate): ReLU(inplace=True)\n",
              "          )\n",
              "          (3): ConvModule(\n",
              "            (conv): Conv2d(32, 16, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
              "            (activate): ReLU(inplace=True)\n",
              "          )\n",
              "          (4): ConvModule(\n",
              "            (conv): Conv2d(16, 2, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (1): SPyNetBasicModule(\n",
              "        (basic_module): Sequential(\n",
              "          (0): ConvModule(\n",
              "            (conv): Conv2d(8, 32, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
              "            (activate): ReLU(inplace=True)\n",
              "          )\n",
              "          (1): ConvModule(\n",
              "            (conv): Conv2d(32, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
              "            (activate): ReLU(inplace=True)\n",
              "          )\n",
              "          (2): ConvModule(\n",
              "            (conv): Conv2d(64, 32, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
              "            (activate): ReLU(inplace=True)\n",
              "          )\n",
              "          (3): ConvModule(\n",
              "            (conv): Conv2d(32, 16, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
              "            (activate): ReLU(inplace=True)\n",
              "          )\n",
              "          (4): ConvModule(\n",
              "            (conv): Conv2d(16, 2, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (2): SPyNetBasicModule(\n",
              "        (basic_module): Sequential(\n",
              "          (0): ConvModule(\n",
              "            (conv): Conv2d(8, 32, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
              "            (activate): ReLU(inplace=True)\n",
              "          )\n",
              "          (1): ConvModule(\n",
              "            (conv): Conv2d(32, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
              "            (activate): ReLU(inplace=True)\n",
              "          )\n",
              "          (2): ConvModule(\n",
              "            (conv): Conv2d(64, 32, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
              "            (activate): ReLU(inplace=True)\n",
              "          )\n",
              "          (3): ConvModule(\n",
              "            (conv): Conv2d(32, 16, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
              "            (activate): ReLU(inplace=True)\n",
              "          )\n",
              "          (4): ConvModule(\n",
              "            (conv): Conv2d(16, 2, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (3): SPyNetBasicModule(\n",
              "        (basic_module): Sequential(\n",
              "          (0): ConvModule(\n",
              "            (conv): Conv2d(8, 32, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
              "            (activate): ReLU(inplace=True)\n",
              "          )\n",
              "          (1): ConvModule(\n",
              "            (conv): Conv2d(32, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
              "            (activate): ReLU(inplace=True)\n",
              "          )\n",
              "          (2): ConvModule(\n",
              "            (conv): Conv2d(64, 32, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
              "            (activate): ReLU(inplace=True)\n",
              "          )\n",
              "          (3): ConvModule(\n",
              "            (conv): Conv2d(32, 16, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
              "            (activate): ReLU(inplace=True)\n",
              "          )\n",
              "          (4): ConvModule(\n",
              "            (conv): Conv2d(16, 2, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (4): SPyNetBasicModule(\n",
              "        (basic_module): Sequential(\n",
              "          (0): ConvModule(\n",
              "            (conv): Conv2d(8, 32, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
              "            (activate): ReLU(inplace=True)\n",
              "          )\n",
              "          (1): ConvModule(\n",
              "            (conv): Conv2d(32, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
              "            (activate): ReLU(inplace=True)\n",
              "          )\n",
              "          (2): ConvModule(\n",
              "            (conv): Conv2d(64, 32, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
              "            (activate): ReLU(inplace=True)\n",
              "          )\n",
              "          (3): ConvModule(\n",
              "            (conv): Conv2d(32, 16, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
              "            (activate): ReLU(inplace=True)\n",
              "          )\n",
              "          (4): ConvModule(\n",
              "            (conv): Conv2d(16, 2, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (5): SPyNetBasicModule(\n",
              "        (basic_module): Sequential(\n",
              "          (0): ConvModule(\n",
              "            (conv): Conv2d(8, 32, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
              "            (activate): ReLU(inplace=True)\n",
              "          )\n",
              "          (1): ConvModule(\n",
              "            (conv): Conv2d(32, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
              "            (activate): ReLU(inplace=True)\n",
              "          )\n",
              "          (2): ConvModule(\n",
              "            (conv): Conv2d(64, 32, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
              "            (activate): ReLU(inplace=True)\n",
              "          )\n",
              "          (3): ConvModule(\n",
              "            (conv): Conv2d(32, 16, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
              "            (activate): ReLU(inplace=True)\n",
              "          )\n",
              "          (4): ConvModule(\n",
              "            (conv): Conv2d(16, 2, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "# set up models\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "net = importlib.import_module('model.' + model_name)\n",
        "model = net.InpaintGenerator().to(device)\n",
        "data = torch.load(ckpt, map_location=device)\n",
        "model.load_state_dict(data)\n",
        "print(f'Loading model from: {ckpt}')\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "video_path_lst = [\n",
        "                  '/content/drive/MyDrive/video_inpating/input/delogo_examples/test_04.mp4',\n",
        "                  '/content/drive/MyDrive/video_inpating/input/delogo_examples/test_05.mp4',\n",
        "                  '/content/drive/MyDrive/video_inpating/input/delogo_examples/west1.mp4',\n",
        "                  '/content/drive/MyDrive/video_inpating/input/delogo_examples/west2.mp4',\n",
        "                  '/content/drive/MyDrive/video_inpating/input/delogo_examples/west3.mp4',\n",
        "                  '/content/drive/MyDrive/video_inpating/input/delogo_examples/west4.mp4',\n",
        "                  '/content/drive/MyDrive/video_inpating/input/delogo_examples/west5.mp4',\n",
        "                  '/content/drive/MyDrive/video_inpating/input/delogo_examples/west6.mp4',\n",
        "]\n",
        "mask_path_lst = [\n",
        "                 '/content/drive/MyDrive/video_inpating/input/delogo_examples/mask/test_04_mask.png',\n",
        "                 '/content/drive/MyDrive/video_inpating/input/delogo_examples/mask/test_05_mask.png',\n",
        "                 '/content/drive/MyDrive/video_inpating/input/delogo_examples/mask/west1_mask.png',\n",
        "                 '/content/drive/MyDrive/video_inpating/input/delogo_examples/mask/west2_mask.png',\n",
        "                 '/content/drive/MyDrive/video_inpating/input/delogo_examples/mask/west3_mask.png',\n",
        "                 '/content/drive/MyDrive/video_inpating/input/delogo_examples/mask/west5_mask.png',\n",
        "                 '/content/drive/MyDrive/video_inpating/input/delogo_examples/mask/west6_mask.png',\n",
        "]\n",
        "\n",
        "for i in range(0, len(video_path_lst)):\n",
        "  video_path = video_path_lst[i]\n",
        "  mask_path = mask_path_lst[i]\n",
        "  # prepare datset\n",
        "  print(\n",
        "      f'Loading videos and masks from: {video_path} | INPUT MP4 format: {use_mp4}'\n",
        "  )\n",
        "  video_length = get_frame_count()\n",
        "  print('video_length={}'.format(video_length))\n",
        "\n",
        "  h, w = size[1], size[0]\n",
        "  comp_frames = [None] * video_length\n",
        "\n",
        "  # completing holes by e2fgvi\n",
        "  print(f'Start test...')\n",
        "  for f in tqdm(range(0, video_length, neighbor_stride)):\n",
        "      neighbor_ids = [\n",
        "          i for i in range(max(0, f - neighbor_stride),\n",
        "                              min(video_length, f + neighbor_stride + 1))\n",
        "      ]\n",
        "      ref_ids = get_ref_index(f, neighbor_ids, video_length)\n",
        "\n",
        "      # read temp imgs and masks\n",
        "      index_lst = neighbor_ids+ref_ids\n",
        "      selected_frames = read_frame_from_videos_by_index_list(index_lst)\n",
        "      selected_frames, size = resize_frames(selected_frames, size)\n",
        "\n",
        "      selected_imgs = to_tensors()(selected_frames).unsqueeze(0) * 2 - 1\n",
        "\n",
        "      selected_frames = [np.array(f).astype(np.uint8) for f in selected_frames]\n",
        "      selected_imgs = selected_imgs.to(device)\n",
        "\n",
        "      if mask_path.endswith('.png'):\n",
        "          selected_masks_data = read_mask_static(mask_path, size, len(index_lst))\n",
        "      else:\n",
        "          selected_masks_data = read_mask_lst(mask_path, size, index_lst)\n",
        "      binary_masks = [\n",
        "          np.expand_dims((np.array(m) != 0).astype(np.uint8), 2) for m in selected_masks_data\n",
        "      ]\n",
        "      selected_masks = to_tensors()(selected_masks_data).unsqueeze(0).to(device)\n",
        "\n",
        "      #selected_imgs = imgs[:1, neighbor_ids + ref_ids, :, :, :].to(device)\n",
        "      #selected_masks = masks[:1, neighbor_ids + ref_ids, :, :, :].to(device)\n",
        "      with torch.no_grad():\n",
        "          masked_imgs = selected_imgs * (1 - selected_masks)\n",
        "          mod_size_h = 60\n",
        "          mod_size_w = 108\n",
        "          h_pad = (mod_size_h - h % mod_size_h) % mod_size_h\n",
        "          w_pad = (mod_size_w - w % mod_size_w) % mod_size_w\n",
        "          masked_imgs = torch.cat(\n",
        "              [masked_imgs, torch.flip(masked_imgs, [3])],\n",
        "              3)[:, :, :, :h + h_pad, :]\n",
        "          masked_imgs = torch.cat(\n",
        "              [masked_imgs, torch.flip(masked_imgs, [4])],\n",
        "              4)[:, :, :, :, :w + w_pad]\n",
        "          pred_imgs, _ = model(masked_imgs, len(neighbor_ids))\n",
        "          pred_imgs = pred_imgs[:, :, :h, :w]\n",
        "          pred_imgs = (pred_imgs + 1) / 2\n",
        "          pred_imgs = pred_imgs.cpu().permute(0, 2, 3, 1).numpy() * 255\n",
        "          for i in range(len(neighbor_ids)):\n",
        "              idx = neighbor_ids[i]\n",
        "              img = np.array(pred_imgs[i]).astype(\n",
        "                  np.uint8) * binary_masks[i] + selected_frames[i] * (\n",
        "                      1 - binary_masks[i])\n",
        "              if comp_frames[idx] is None:\n",
        "                  comp_frames[idx] = img\n",
        "              else:\n",
        "                  comp_frames[idx] = comp_frames[idx].astype(\n",
        "                      np.float32) * 0.5 + img.astype(np.float32) * 0.5\n",
        "\n",
        "  print('Saving videos...')\n",
        "  save_dir_name = '/content/drive/MyDrive/video_inpating/results'\n",
        "  ext_name = '_results.mp4'\n",
        "  save_base_name = video_path.split('/')[-1]\n",
        "  save_name = save_base_name.replace(\n",
        "      '.mp4', ext_name) if use_mp4 else save_base_name + ext_name\n",
        "  if not os.path.exists(save_dir_name):\n",
        "      os.makedirs(save_dir_name)\n",
        "  save_path = os.path.join(save_dir_name, save_name)\n",
        "  writer = cv2.VideoWriter(save_path, cv2.VideoWriter_fourcc(*\"mp4v\"),\n",
        "                              default_fps, size)\n",
        "  for f in range(video_length):\n",
        "      comp = comp_frames[f].astype(np.uint8)\n",
        "      writer.write(cv2.cvtColor(comp, cv2.COLOR_BGR2RGB))\n",
        "  writer.release()\n",
        "  print(f'Finish test! The result video is saved in: {save_path}.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tlaO0T56i3t6",
        "outputId": "d0505e53-bf0a-4773-ca39-92f2e88614aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading videos and masks from: /content/drive/MyDrive/video_inpating/input/delogo_examples/test_04.mp4 | INPUT MP4 format: True\n",
            "video_length=576\n",
            "Start test...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  2%|▏         | 4/192 [00:11<09:11,  2.94s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6GwfO29WipV"
      },
      "source": [
        "## Write output to drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X-HAxpdUWltr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "785db875-5a18-4f7a-a188-706ef986d407"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving videos...\n",
            "Finish test! The result video is saved in: /content/drive/MyDrive/video_inpating/results/test_03_results.mp4.\n"
          ]
        }
      ],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "E2FGVI.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "704c4b9337104692a4776c200bae12e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4b78baf38baa4d2db64899361ae29965",
              "IPY_MODEL_e8c471a04e3046109811b42f47f80c88",
              "IPY_MODEL_4cecdec88c774842b63760cb6d398bb7"
            ],
            "layout": "IPY_MODEL_e33fd62ea20847f7a469df1a52c45bb0"
          }
        },
        "4b78baf38baa4d2db64899361ae29965": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_15e53b8a29d04fa488381b5b3916d4b1",
            "placeholder": "​",
            "style": "IPY_MODEL_206ef8d2643a4c09934890197b019527",
            "value": "100%"
          }
        },
        "e8c471a04e3046109811b42f47f80c88": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b3f7d7a4aae84163803e83f729d072ca",
            "max": 5770988,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9720bcbfa95f4eb9a022ce037d742faf",
            "value": 5770988
          }
        },
        "4cecdec88c774842b63760cb6d398bb7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f0d71edb91814477b192a6ca97ac58f7",
            "placeholder": "​",
            "style": "IPY_MODEL_faa7a70ea0a045f68314f8cdcedabf17",
            "value": " 5.50M/5.50M [00:01&lt;00:00, 8.69MB/s]"
          }
        },
        "e33fd62ea20847f7a469df1a52c45bb0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "15e53b8a29d04fa488381b5b3916d4b1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "206ef8d2643a4c09934890197b019527": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b3f7d7a4aae84163803e83f729d072ca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9720bcbfa95f4eb9a022ce037d742faf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f0d71edb91814477b192a6ca97ac58f7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "faa7a70ea0a045f68314f8cdcedabf17": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}